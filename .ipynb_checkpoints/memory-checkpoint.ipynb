{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  HumanMessage {\n",
      "    lc_serializable: \u001b[33mtrue\u001b[39m,\n",
      "    lc_kwargs: { content: \u001b[32m\"hi\"\u001b[39m, additional_kwargs: {}, response_metadata: {} },\n",
      "    lc_namespace: [ \u001b[32m\"langchain_core\"\u001b[39m, \u001b[32m\"messages\"\u001b[39m ],\n",
      "    content: \u001b[32m\"hi\"\u001b[39m,\n",
      "    name: \u001b[90mundefined\u001b[39m,\n",
      "    additional_kwargs: {},\n",
      "    response_metadata: {}\n",
      "  },\n",
      "  AIMessage {\n",
      "    lc_serializable: \u001b[33mtrue\u001b[39m,\n",
      "    lc_kwargs: {\n",
      "      content: \u001b[32m\"What can I do for you?\"\u001b[39m,\n",
      "      additional_kwargs: {},\n",
      "      response_metadata: {}\n",
      "    },\n",
      "    lc_namespace: [ \u001b[32m\"langchain_core\"\u001b[39m, \u001b[32m\"messages\"\u001b[39m ],\n",
      "    content: \u001b[32m\"What can I do for you?\"\u001b[39m,\n",
      "    name: \u001b[90mundefined\u001b[39m,\n",
      "    additional_kwargs: {},\n",
      "    response_metadata: {}\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import { ChatMessageHistory } from \"langchain/stores/message/in_memory\";\n",
    "import { HumanMessage, AIMessage } from \"@langchain/core/messages\";\n",
    "\n",
    "const history = new ChatMessageHistory();\n",
    "\n",
    "await history.addMessage(new HumanMessage(\"hi\"));\n",
    "await history.addMessage(new AIMessage(\"What can I do for you?\"));\n",
    "\n",
    "const messages = await history.getMessages();\n",
    "\n",
    "console.log(messages);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { load } from \"dotenv\";\n",
    "const env = await load();\n",
    "const process = {\n",
    "    env\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { ChatPromptTemplate, MessagesPlaceholder } from \"@langchain/core/prompts\";\n",
    "import { ChatAlibabaTongyi } from \"@langchain/community/chat_models/alibaba_tongyi\";\n",
    "\n",
    "const chatModel = new ChatAlibabaTongyi({\n",
    "    model: \"qwen-turbo\", // Available models: qwen-turbo, qwen-plus, qwen-max\n",
    "    temperature: 1,\n",
    "});\n",
    "\n",
    "const prompt = ChatPromptTemplate.fromMessages([\n",
    "    [\"system\", `You are a helpful assistant. Answer all questions to the best of your ability.\n",
    "    You are talkative and provides lots of specific details from its context. \n",
    "    If the you does not know the answer to a question, it truthfully says you do not know.`],\n",
    "    new MessagesPlaceholder(\"history_message\"),\n",
    "]);\n",
    "\n",
    "const chain = prompt.pipe(chatModel);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AIMessage {\n",
      "  lc_serializable: \u001b[33mtrue\u001b[39m,\n",
      "  lc_kwargs: {\n",
      "    content: \u001b[32m\"Hello Kai! Nice to meet you. Is there anything specific you'd like to chat about or ask? I'm here to\"\u001b[39m... 53 more characters,\n",
      "    tool_calls: [],\n",
      "    invalid_tool_calls: [],\n",
      "    additional_kwargs: {},\n",
      "    response_metadata: {}\n",
      "  },\n",
      "  lc_namespace: [ \u001b[32m\"langchain_core\"\u001b[39m, \u001b[32m\"messages\"\u001b[39m ],\n",
      "  content: \u001b[32m\"Hello Kai! Nice to meet you. Is there anything specific you'd like to chat about or ask? I'm here to\"\u001b[39m... 53 more characters,\n",
      "  name: \u001b[90mundefined\u001b[39m,\n",
      "  additional_kwargs: {},\n",
      "  response_metadata: {\n",
      "    tokenUsage: { promptTokens: \u001b[33m73\u001b[39m, completionTokens: \u001b[33m34\u001b[39m, totalTokens: \u001b[33m107\u001b[39m }\n",
      "  },\n",
      "  tool_calls: [],\n",
      "  invalid_tool_calls: []\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import { ChatMessageHistory } from \"langchain/stores/message/in_memory\";\n",
    "import { HumanMessage, AIMessage } from \"@langchain/core/messages\";\n",
    "\n",
    "// process.env.LANGCHAIN_VERBOSE = \"false\";\n",
    "\n",
    "const history = new ChatMessageHistory();\n",
    "await history.addMessage(new HumanMessage(\"hi, my name is Kai\"));\n",
    "\n",
    "const res1 = await chain.invoke({\n",
    "    history_message: await history.getMessages()\n",
    "});\n",
    "\n",
    "console.log(res1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "await history.addMessage(res1)\n",
    "await history.addMessage(new HumanMessage(\"What is my name?\"));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AIMessage {\n",
      "  lc_serializable: \u001b[33mtrue\u001b[39m,\n",
      "  lc_kwargs: {\n",
      "    content: \u001b[32m\"Your name is Kai.\"\u001b[39m,\n",
      "    tool_calls: [],\n",
      "    invalid_tool_calls: [],\n",
      "    additional_kwargs: {},\n",
      "    response_metadata: {}\n",
      "  },\n",
      "  lc_namespace: [ \u001b[32m\"langchain_core\"\u001b[39m, \u001b[32m\"messages\"\u001b[39m ],\n",
      "  content: \u001b[32m\"Your name is Kai.\"\u001b[39m,\n",
      "  name: \u001b[90mundefined\u001b[39m,\n",
      "  additional_kwargs: {},\n",
      "  response_metadata: {\n",
      "    tokenUsage: { promptTokens: \u001b[33m122\u001b[39m, completionTokens: \u001b[33m5\u001b[39m, totalTokens: \u001b[33m127\u001b[39m }\n",
      "  },\n",
      "  tool_calls: [],\n",
      "  invalid_tool_calls: []\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "const res2 = await chain.invoke({\n",
    "    history_message: await history.getMessages()\n",
    "});\n",
    "console.log(res2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { Ollama } from \"@langchain/community/llms/ollama\";\n",
    "import { ChatMessageHistory } from \"langchain/stores/message/in_memory\";\n",
    "import { RunnableWithMessageHistory } from \"@langchain/core/runnables\";\n",
    "\n",
    "const chatModel = new Ollama({\n",
    "  baseUrl: \"http://localhost:11434\", \n",
    "  model: \"qwen:7b\", \n",
    "});\n",
    "\n",
    "const prompt = ChatPromptTemplate.fromMessages([\n",
    "    [\"system\", \"You are a helpful assistant. Answer all questions to the best of your ability.\"],\n",
    "    new MessagesPlaceholder(\"history_message\"),\n",
    "    [\"human\",\"{input}\"]\n",
    "]);\n",
    "\n",
    "const history = new ChatMessageHistory();\n",
    "const chain = prompt.pipe(chatModel)\n",
    "\n",
    "const chainWithHistory = new RunnableWithMessageHistory({\n",
    "  runnable: chain,\n",
    "  getMessageHistory: (_sessionId) => history,\n",
    "  inputMessagesKey: \"input\",\n",
    "  historyMessagesKey: \"history_message\",\n",
    "});"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Kai! Nice to meet you. How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "const res1 = await chainWithHistory.invoke({\n",
    "    input: \"hi, my name is Kai\"\n",
    "},{\n",
    "    configurable: { sessionId: \"none\" }\n",
    "});\n",
    "console.log(res1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "您的名字是Kai。\n"
     ]
    }
   ],
   "source": [
    "const res2 = await chainWithHistory.invoke({\n",
    "    input: \"我的名字叫什么？\"\n",
    "},{\n",
    "    configurable: { sessionId: \"none\" }\n",
    "});\n",
    "console.log(res2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  HumanMessage {\n",
      "    lc_serializable: \u001b[33mtrue\u001b[39m,\n",
      "    lc_kwargs: {\n",
      "      content: \u001b[32m\"hi, my name is Kai\"\u001b[39m,\n",
      "      additional_kwargs: {},\n",
      "      response_metadata: {}\n",
      "    },\n",
      "    lc_namespace: [ \u001b[32m\"langchain_core\"\u001b[39m, \u001b[32m\"messages\"\u001b[39m ],\n",
      "    content: \u001b[32m\"hi, my name is Kai\"\u001b[39m,\n",
      "    name: \u001b[90mundefined\u001b[39m,\n",
      "    additional_kwargs: {},\n",
      "    response_metadata: {}\n",
      "  },\n",
      "  AIMessage {\n",
      "    lc_serializable: \u001b[33mtrue\u001b[39m,\n",
      "    lc_kwargs: {\n",
      "      content: \u001b[32m\"Hello Kai! Nice to meet you. How can I assist you today?\"\u001b[39m,\n",
      "      additional_kwargs: {},\n",
      "      response_metadata: {}\n",
      "    },\n",
      "    lc_namespace: [ \u001b[32m\"langchain_core\"\u001b[39m, \u001b[32m\"messages\"\u001b[39m ],\n",
      "    content: \u001b[32m\"Hello Kai! Nice to meet you. How can I assist you today?\"\u001b[39m,\n",
      "    name: \u001b[90mundefined\u001b[39m,\n",
      "    additional_kwargs: {},\n",
      "    response_metadata: {}\n",
      "  },\n",
      "  HumanMessage {\n",
      "    lc_serializable: \u001b[33mtrue\u001b[39m,\n",
      "    lc_kwargs: {\n",
      "      content: \u001b[32m\"我的名字叫什么？\"\u001b[39m,\n",
      "      additional_kwargs: {},\n",
      "      response_metadata: {}\n",
      "    },\n",
      "    lc_namespace: [ \u001b[32m\"langchain_core\"\u001b[39m, \u001b[32m\"messages\"\u001b[39m ],\n",
      "    content: \u001b[32m\"我的名字叫什么？\"\u001b[39m,\n",
      "    name: \u001b[90mundefined\u001b[39m,\n",
      "    additional_kwargs: {},\n",
      "    response_metadata: {}\n",
      "  },\n",
      "  AIMessage {\n",
      "    lc_serializable: \u001b[33mtrue\u001b[39m,\n",
      "    lc_kwargs: {\n",
      "      content: \u001b[32m\"您的名字是Kai。\"\u001b[39m,\n",
      "      additional_kwargs: {},\n",
      "      response_metadata: {}\n",
      "    },\n",
      "    lc_namespace: [ \u001b[32m\"langchain_core\"\u001b[39m, \u001b[32m\"messages\"\u001b[39m ],\n",
      "    content: \u001b[32m\"您的名字是Kai。\"\u001b[39m,\n",
      "    name: \u001b[90mundefined\u001b[39m,\n",
      "    additional_kwargs: {},\n",
      "    response_metadata: {}\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "const msg = await history.getMessages();\n",
    "console.log(msg);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { Ollama } from \"@langchain/community/llms/ollama\";\n",
    "import { RunnableSequence } from \"@langchain/core/runnables\";\n",
    "import { StringOutputParser } from \"@langchain/core/output_parsers\";\n",
    "\n",
    "const summaryModel = new Ollama({\n",
    "  baseUrl: \"http://localhost:11434\", \n",
    "  model: \"qwen:7b\", \n",
    "});\n",
    "\n",
    "const summaryPrompt = ChatPromptTemplate.fromTemplate(`\n",
    "Progressively summarize the lines of conversation provided, adding onto the previous summary returning a new summary\n",
    "\n",
    "Current summary:\n",
    "{summary}\n",
    "\n",
    "New lines of conversation:\n",
    "{new_lines}\n",
    "\n",
    "New summary:\n",
    "`); \n",
    "\n",
    "const summaryChain = RunnableSequence.from([\n",
    "    summaryPrompt,\n",
    "    summaryModel,\n",
    "    new StringOutputParser(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "ReferenceError",
     "evalue": "newSummary is not defined",
     "output_type": "error",
     "traceback": [
      "Stack trace:",
      "ReferenceError: newSummary is not defined",
      "    at <anonymous>:2:12"
     ]
    }
   ],
   "source": [
    "const newSummary = await summaryChain.invoke({\n",
    "    summary: newSummary,\n",
    "    new_lines: \"I'm male\"\n",
    "});"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Deno",
   "language": "typescript",
   "name": "deno"
  },
  "language_info": {
   "file_extension": ".ts",
   "mimetype": "text/x.typescript",
   "name": "typescript",
   "nb_converter": "script",
   "pygments_lexer": "typescript",
   "version": "5.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
