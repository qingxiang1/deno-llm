{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  HumanMessage {\n",
      "    lc_serializable: \u001b[33mtrue\u001b[39m,\n",
      "    lc_kwargs: { content: \u001b[32m\"hi\"\u001b[39m, additional_kwargs: {}, response_metadata: {} },\n",
      "    lc_namespace: [ \u001b[32m\"langchain_core\"\u001b[39m, \u001b[32m\"messages\"\u001b[39m ],\n",
      "    content: \u001b[32m\"hi\"\u001b[39m,\n",
      "    name: \u001b[90mundefined\u001b[39m,\n",
      "    additional_kwargs: {},\n",
      "    response_metadata: {}\n",
      "  },\n",
      "  AIMessage {\n",
      "    lc_serializable: \u001b[33mtrue\u001b[39m,\n",
      "    lc_kwargs: {\n",
      "      content: \u001b[32m\"What can I do for you?\"\u001b[39m,\n",
      "      additional_kwargs: {},\n",
      "      response_metadata: {}\n",
      "    },\n",
      "    lc_namespace: [ \u001b[32m\"langchain_core\"\u001b[39m, \u001b[32m\"messages\"\u001b[39m ],\n",
      "    content: \u001b[32m\"What can I do for you?\"\u001b[39m,\n",
      "    name: \u001b[90mundefined\u001b[39m,\n",
      "    additional_kwargs: {},\n",
      "    response_metadata: {}\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import { ChatMessageHistory } from \"langchain/stores/message/in_memory\";\n",
    "import { HumanMessage, AIMessage } from \"@langchain/core/messages\";\n",
    "\n",
    "const history = new ChatMessageHistory();\n",
    "\n",
    "await history.addMessage(new HumanMessage(\"hi\"));\n",
    "await history.addMessage(new AIMessage(\"What can I do for you?\"));\n",
    "\n",
    "const messages = await history.getMessages();\n",
    "\n",
    "console.log(messages);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { load } from \"dotenv\";\n",
    "const env = await load();\n",
    "const process = {\n",
    "    env\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { ChatPromptTemplate, MessagesPlaceholder } from \"@langchain/core/prompts\";\n",
    "import { ChatAlibabaTongyi } from \"@langchain/community/chat_models/alibaba_tongyi\";\n",
    "\n",
    "const chatModel = new ChatAlibabaTongyi({\n",
    "    model: \"qwen-turbo\", // Available models: qwen-turbo, qwen-plus, qwen-max\n",
    "    temperature: 1,\n",
    "});\n",
    "\n",
    "const prompt = ChatPromptTemplate.fromMessages([\n",
    "    [\"system\", `You are a helpful assistant. Answer all questions to the best of your ability.\n",
    "    You are talkative and provides lots of specific details from its context. \n",
    "    If the you does not know the answer to a question, it truthfully says you do not know.`],\n",
    "    new MessagesPlaceholder(\"history_message\"),\n",
    "]);\n",
    "\n",
    "const chain = prompt.pipe(chatModel);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AIMessage {\n",
      "  lc_serializable: \u001b[33mtrue\u001b[39m,\n",
      "  lc_kwargs: {\n",
      "    content: \u001b[32m\"Hello Kai! Nice to meet you. Is there anything specific you'd like to chat about or ask? I'm here to\"\u001b[39m... 53 more characters,\n",
      "    tool_calls: [],\n",
      "    invalid_tool_calls: [],\n",
      "    additional_kwargs: {},\n",
      "    response_metadata: {}\n",
      "  },\n",
      "  lc_namespace: [ \u001b[32m\"langchain_core\"\u001b[39m, \u001b[32m\"messages\"\u001b[39m ],\n",
      "  content: \u001b[32m\"Hello Kai! Nice to meet you. Is there anything specific you'd like to chat about or ask? I'm here to\"\u001b[39m... 53 more characters,\n",
      "  name: \u001b[90mundefined\u001b[39m,\n",
      "  additional_kwargs: {},\n",
      "  response_metadata: {\n",
      "    tokenUsage: { promptTokens: \u001b[33m73\u001b[39m, completionTokens: \u001b[33m34\u001b[39m, totalTokens: \u001b[33m107\u001b[39m }\n",
      "  },\n",
      "  tool_calls: [],\n",
      "  invalid_tool_calls: []\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import { ChatMessageHistory } from \"langchain/stores/message/in_memory\";\n",
    "import { HumanMessage, AIMessage } from \"@langchain/core/messages\";\n",
    "\n",
    "// process.env.LANGCHAIN_VERBOSE = \"false\";\n",
    "\n",
    "const history = new ChatMessageHistory();\n",
    "await history.addMessage(new HumanMessage(\"hi, my name is Kai\"));\n",
    "\n",
    "const res1 = await chain.invoke({\n",
    "    history_message: await history.getMessages()\n",
    "});\n",
    "\n",
    "console.log(res1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "await history.addMessage(res1)\n",
    "await history.addMessage(new HumanMessage(\"What is my name?\"));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AIMessage {\n",
      "  lc_serializable: \u001b[33mtrue\u001b[39m,\n",
      "  lc_kwargs: {\n",
      "    content: \u001b[32m\"Your name is Kai.\"\u001b[39m,\n",
      "    tool_calls: [],\n",
      "    invalid_tool_calls: [],\n",
      "    additional_kwargs: {},\n",
      "    response_metadata: {}\n",
      "  },\n",
      "  lc_namespace: [ \u001b[32m\"langchain_core\"\u001b[39m, \u001b[32m\"messages\"\u001b[39m ],\n",
      "  content: \u001b[32m\"Your name is Kai.\"\u001b[39m,\n",
      "  name: \u001b[90mundefined\u001b[39m,\n",
      "  additional_kwargs: {},\n",
      "  response_metadata: {\n",
      "    tokenUsage: { promptTokens: \u001b[33m122\u001b[39m, completionTokens: \u001b[33m5\u001b[39m, totalTokens: \u001b[33m127\u001b[39m }\n",
      "  },\n",
      "  tool_calls: [],\n",
      "  invalid_tool_calls: []\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "const res2 = await chain.invoke({\n",
    "    history_message: await history.getMessages()\n",
    "});\n",
    "console.log(res2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { Ollama } from \"@langchain/community/llms/ollama\";\n",
    "import { ChatMessageHistory } from \"langchain/stores/message/in_memory\";\n",
    "import { RunnableWithMessageHistory } from \"@langchain/core/runnables\";\n",
    "\n",
    "const chatModel = new Ollama({\n",
    "  baseUrl: \"http://localhost:11434\", \n",
    "  model: \"qwen:7b\", \n",
    "});\n",
    "\n",
    "const prompt = ChatPromptTemplate.fromMessages([\n",
    "    [\"system\", \"You are a helpful assistant. Answer all questions to the best of your ability.\"],\n",
    "    new MessagesPlaceholder(\"history_message\"),\n",
    "    [\"human\",\"{input}\"]\n",
    "]);\n",
    "\n",
    "const history = new ChatMessageHistory();\n",
    "const chain = prompt.pipe(chatModel)\n",
    "\n",
    "const chainWithHistory = new RunnableWithMessageHistory({\n",
    "  runnable: chain,\n",
    "  getMessageHistory: (_sessionId) => history,\n",
    "  inputMessagesKey: \"input\",\n",
    "  historyMessagesKey: \"history_message\",\n",
    "});"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Kai! Nice to meet you. How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "const res1 = await chainWithHistory.invoke({\n",
    "    input: \"hi, my name is Kai\"\n",
    "},{\n",
    "    configurable: { sessionId: \"none\" }\n",
    "});\n",
    "console.log(res1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "您的名字是Kai。\n"
     ]
    }
   ],
   "source": [
    "const res2 = await chainWithHistory.invoke({\n",
    "    input: \"我的名字叫什么？\"\n",
    "},{\n",
    "    configurable: { sessionId: \"none\" }\n",
    "});\n",
    "console.log(res2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  HumanMessage {\n",
      "    lc_serializable: \u001b[33mtrue\u001b[39m,\n",
      "    lc_kwargs: {\n",
      "      content: \u001b[32m\"hi, my name is Kai\"\u001b[39m,\n",
      "      additional_kwargs: {},\n",
      "      response_metadata: {}\n",
      "    },\n",
      "    lc_namespace: [ \u001b[32m\"langchain_core\"\u001b[39m, \u001b[32m\"messages\"\u001b[39m ],\n",
      "    content: \u001b[32m\"hi, my name is Kai\"\u001b[39m,\n",
      "    name: \u001b[90mundefined\u001b[39m,\n",
      "    additional_kwargs: {},\n",
      "    response_metadata: {}\n",
      "  },\n",
      "  AIMessage {\n",
      "    lc_serializable: \u001b[33mtrue\u001b[39m,\n",
      "    lc_kwargs: {\n",
      "      content: \u001b[32m\"Hello Kai! Nice to meet you. How can I assist you today?\"\u001b[39m,\n",
      "      additional_kwargs: {},\n",
      "      response_metadata: {}\n",
      "    },\n",
      "    lc_namespace: [ \u001b[32m\"langchain_core\"\u001b[39m, \u001b[32m\"messages\"\u001b[39m ],\n",
      "    content: \u001b[32m\"Hello Kai! Nice to meet you. How can I assist you today?\"\u001b[39m,\n",
      "    name: \u001b[90mundefined\u001b[39m,\n",
      "    additional_kwargs: {},\n",
      "    response_metadata: {}\n",
      "  },\n",
      "  HumanMessage {\n",
      "    lc_serializable: \u001b[33mtrue\u001b[39m,\n",
      "    lc_kwargs: {\n",
      "      content: \u001b[32m\"我的名字叫什么？\"\u001b[39m,\n",
      "      additional_kwargs: {},\n",
      "      response_metadata: {}\n",
      "    },\n",
      "    lc_namespace: [ \u001b[32m\"langchain_core\"\u001b[39m, \u001b[32m\"messages\"\u001b[39m ],\n",
      "    content: \u001b[32m\"我的名字叫什么？\"\u001b[39m,\n",
      "    name: \u001b[90mundefined\u001b[39m,\n",
      "    additional_kwargs: {},\n",
      "    response_metadata: {}\n",
      "  },\n",
      "  AIMessage {\n",
      "    lc_serializable: \u001b[33mtrue\u001b[39m,\n",
      "    lc_kwargs: {\n",
      "      content: \u001b[32m\"您的名字是Kai。\"\u001b[39m,\n",
      "      additional_kwargs: {},\n",
      "      response_metadata: {}\n",
      "    },\n",
      "    lc_namespace: [ \u001b[32m\"langchain_core\"\u001b[39m, \u001b[32m\"messages\"\u001b[39m ],\n",
      "    content: \u001b[32m\"您的名字是Kai。\"\u001b[39m,\n",
      "    name: \u001b[90mundefined\u001b[39m,\n",
      "    additional_kwargs: {},\n",
      "    response_metadata: {}\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "const msg = await history.getMessages();\n",
    "console.log(msg);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { Ollama } from \"@langchain/community/llms/ollama\";\n",
    "import { RunnableSequence } from \"@langchain/core/runnables\";\n",
    "import { StringOutputParser } from \"@langchain/core/output_parsers\";\n",
    "\n",
    "const summaryModel = new Ollama({\n",
    "  baseUrl: \"http://localhost:11434\", \n",
    "  model: \"qwen:7b\", \n",
    "});\n",
    "\n",
    "const summaryPrompt = ChatPromptTemplate.fromTemplate(`\n",
    "Progressively summarize the lines of conversation provided, adding onto the previous summary returning a new summary\n",
    "\n",
    "Current summary:\n",
    "{summary}\n",
    "\n",
    "New lines of conversation:\n",
    "{new_lines}\n",
    "\n",
    "New summary:\n",
    "`); \n",
    "\n",
    "const summaryChain = RunnableSequence.from([\n",
    "    summaryPrompt,\n",
    "    summaryModel,\n",
    "    new StringOutputParser(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "const newSummary = await summaryChain.invoke({\n",
    "    summary: \"\",\n",
    "    new_lines: \"I'm 18\"\n",
    "});"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32m\"At 18 years old, the individual shares their age and gender.\"\u001b[39m"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await summaryChain.invoke({\n",
    "    summary: newSummary,\n",
    "    new_lines: \"I'm male\"\n",
    "});"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "const chatModel = new Ollama({\n",
    "  baseUrl: \"http://localhost:11434\", \n",
    "  model: \"qwen:7b\", \n",
    "});\n",
    "const chatPrompt = ChatPromptTemplate.fromMessages([\n",
    "    [\"system\", `You are a helpful assistant. Answer all questions to the best of your ability.\n",
    "\n",
    "    Here is the chat history summary:\n",
    "    {history_summary}\n",
    "    `],\n",
    "    [\"human\",\"{input}\"]\n",
    "]);\n",
    "let summary = \"\"\n",
    "const history = new ChatMessageHistory();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { RunnablePassthrough } from \"@langchain/core/runnables\";\n",
    "import { getBufferString } from \"@langchain/core/messages\";\n",
    "\n",
    "const chatChain = RunnableSequence.from([\n",
    "    {\n",
    "        input: new RunnablePassthrough({\n",
    "             func: (input) => history.addUserMessage(input)\n",
    "        })\n",
    "    },\n",
    "    RunnablePassthrough.assign({\n",
    "        history_summary: () => summary\n",
    "    }),\n",
    "    chatPrompt,\n",
    "    chatModel,\n",
    "    new StringOutputParser(),\n",
    "    new RunnablePassthrough({\n",
    "        func: async (input) => {\n",
    "            history.addAIChatMessage(input)\n",
    "            const messages = await history.getMessages()\n",
    "            const new_lines = getBufferString(messages)\n",
    "            const newSummary = await summaryChain.invoke({\n",
    "                summary,\n",
    "                new_lines\n",
    "            })\n",
    "            history.clear()\n",
    "            summary = newSummary      \n",
    "        }\n",
    "    })\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ a: \u001b[32m\"a\"\u001b[39m, b: \u001b[32m\"b\"\u001b[39m }\n"
     ]
    }
   ],
   "source": [
    "import { RunnableMap } from \"@langchain/core/runnables\";\n",
    "\n",
    "const mapChain = RunnableMap.from({\n",
    "    a: () => \"a\",\n",
    "    b: () => \"b\"\n",
    "});\n",
    "\n",
    "const res = await mapChain.invoke();\n",
    "console.log(res);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32m\"我理解您感到饥饿。如果您在家中，可能需要准备一些食物来满足饥饿感。比如，您可以吃些坚果、水果，或者做一些简单的饭菜。\\n\"\u001b[39m +\n",
       "  \u001b[32m\"\\n\"\u001b[39m +\n",
       "  \u001b[32m\"如果在外面或者不方便自己准备，可以考虑附近是否有餐厅或便利店提供食品服务。也可以\"\u001b[39m... 41 more characters"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await chatChain.invoke(\"我现在饿了\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "如果你想吃方便面，可以按照以下方法来解决：\n",
      "\n",
      "1. **确认库存**：首先检查家里是否还有剩余的方便面。如果有足够的量，可以直接使用。\n",
      "\n",
      "2. **购买新包装**：如果家中没有方便面，需要去附近的便利店或超市购买新的方便面包装。\n",
      "\n",
      "3. **准备泡面**：在买到方便面后，拆开包装并通常需要用热水来煮面。注意水温不要过高，以免破坏面条的口感。\n",
      "\n",
      "4. **添加调料**：煮好的方便面可以根据个人口味添加调料，如酱油、油辣酱、蔬菜等。\n",
      "\n",
      "按照这些步骤，你就可以轻松享用一碗美味的方便面了。\n"
     ]
    }
   ],
   "source": [
    "const res = await chatChain.invoke(\"我今天想吃方便面\");\n",
    "console.log(res);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Deno",
   "language": "typescript",
   "name": "deno"
  },
  "language_info": {
   "file_extension": ".ts",
   "mimetype": "text/x.typescript",
   "name": "typescript",
   "nb_converter": "script",
   "pygments_lexer": "typescript",
   "version": "5.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
